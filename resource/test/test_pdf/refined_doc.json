{
    "sections": [
        {
            "title": "*PPTAgent*: Generating and Evaluating Presentations Beyond Text-to-Slides",
            "subsections": []
        },
        {
            "title": "Abstract",
            "subsections": [
                {
                    "title": "Introduction to PPTAgent",
                    "content": "Automatically generating presentations from documents is a challenging task that requires balancing content quality, visual design, and structural coherence. Existing methods primarily focus on improving and evaluating the content quality in isolation, often overlooking visual design and structural coherence, which limits their practical applicability. To address these limitations, we propose *PPTAgent*, which comprehensively improves presentation generation through a two-stage, edit-based approach inspired by human workflows.",
                    "medias": []
                },
                {
                    "title": "PPTAgent Workflow",
                    "content": "*PPTAgent* first analyzes reference presentations to understand their structural patterns and content schemas, then drafts outlines and generates slides through code actions to ensure consistency and alignment.",
                    "medias": []
                },
                {
                    "title": "Evaluation Framework",
                    "content": "To comprehensively evaluate the quality of generated presentations, we further introduce *PPTEval*, an evaluation framework that assesses presentations across three dimensions: Content, Design, and Coherence. Experiments show that *PPTAgent* significantly outperforms traditional automatic presentation generation methods across all three dimensions.",
                    "medias": []
                },
                {
                    "title": "Availability",
                    "content": "The code and data are available at https://github.com/icip-cas/PPTAgent.",
                    "medias": []
                }
            ]
        },
        {
            "title": "1 Introduction",
            "subsections": [
                {
                    "title": "Challenges in Presentation Creation",
                    "content": "Presentations are a widely used medium for information delivery, valued for their visual effectiveness in engaging and communicating with audiences. Creating high-quality presentations requires a captivating storyline, visually appealing layouts, and rich, impactful content. Consequently, creating well-rounded presentations requires advanced presentation skills and significant effort. There is growing interest in automating the presentation generation process by leveraging the generalization capabilities of large language models (LLM).",
                    "medias": []
                },
                {
                    "title": "Comparison with Conventional Methods",
                    "content": "Existing approaches often adopt an end-to-end text-generation paradigm, focusing solely on textual content while neglecting layout design and presentation structures, making them impractical for real-world applications. For example, prior studies treat presentation generation as an abstractive summarization task, focusing primarily on textual content while overlooking the interactive nature of presentations. This results in simplistic and visually uninspiring outputs that fail to engage audiences.",
                    "medias": [
                        {
                            "markdown_content": "![](_page_0_Figure_14.jpeg)",
                            "markdown_caption": "Figure 1: Comparison between our*PPTAgent* approach (left) and the conventional abstractive summarization method (right). Our method, which begins by editing a reference slide, aligns more closely with the human presentation creation process.",
                            "path": "test_pdf/_page_0_Figure_14.jpeg",
                            "caption": "Diagram: Compares PPTAgent's edit-based generation with abstractive summarization methods. Highlights advantages like visual support and engaging design versus tedious text and boring layout. Focuses on AI era content.\n\n"
                        }
                    ]
                },
                {
                    "title": "Challenges in Automated Presentation Generation",
                    "content": "Automatically creating visually rich and structurally clear presentations remains challenging due to the complexity of data formats and the lack of effective evaluation frameworks. First, most presentations are saved in PowerPoint's XML format, which is inherently tedious and redundant. This complex format poses significant challenges for LLMs in interpreting the presentation layout and structure, let alone generating appealing slides in an end-to-end fashion. Second, the absence of comprehensive evaluation frameworks exacerbates this issue. Current metrics like perplexity and ROUGE fail to capture essential aspects of presentation quality such as narrative flow, visual design, and content impact. Moreover, ROUGE-based evaluation tends to reward excessive textual alignment with input documents, undermining the brevity and clarity crucial for effective presentations. These limitations highlight the urgent need for advancements in automated presentation generation, particularly in enhancing visual design and developing comprehensive evaluation frameworks.",
                    "medias": []
                },
                {
                    "title": "PPTAgent Workflow",
                    "content": "Rather than creating complex presentations from scratch in a single pass, presentations are typically created by selecting exemplary slides as references and then summarizing and transferring key content onto them. Inspired by this process, we design *PPTAgent* to decompose presentation generation into an iterative, edit-based workflow. In the first stage, given a document and a reference presentation, *PPTAgent* analyzes the reference presentations to extract semantic information, providing the textual description that identifies the purpose and data model of each slide. In the Presentation Generation stage, *PPTAgent* generates a detailed presentation outline and assigns specific document sections and reference slides to each slide. For instance, the framework selects the opening slide as the reference slide to present meta-information, such as the title and icon. *PPTAgent* offers a suite of editing action APIs that empower LLMs to dynamically modify the reference slide. By breaking down the process into discrete stages rather than end-to-end generation, this approach ensures consistency, adaptability, and seamless handling of complex formats.",
                    "medias": [
                        {
                            "markdown_content": "![](_page_1_Figure_0.jpeg)",
                            "markdown_caption": "Figure 2: Overview of the *PPTAgent* workflow. *Stage I: Presentation Analysis* involves analyzing the input presentation to cluster slides into groups and extract their content schemas. *Stage II: Presentation Generation* generates new presentations guided by the outline, incorporating feedback mechanisms to ensure robustness.",
                            "path": "test_pdf/_page_1_Figure_0.jpeg",
                            "caption": "Diagram: Workflow illustrating PPTAgent's process from slide clustering and schema extraction to outline and slide generation, including feedback and self-correction mechanisms for presentation creation.\n\n"
                        }
                    ]
                },
                {
                    "title": "Evaluation and Contributions",
                    "content": "To comprehensively evaluate the quality of generated presentations, we propose *PPTEval*, a multidimensional evaluation framework. *PPTEval* leverages the MLLM-as-a-judge paradigm to enable systematic and scalable evaluation, providing both quantitative scores and qualitative feedback for each dimension. Our human evaluation studies validated the reliability and effectiveness of *PPTEval*. Results demonstrate that our method effectively generates high-quality presentations, achieving an average score of 3.67 across the three dimensions evaluated by *PPTEval*. These results, covering a diverse range of domains, highlight a high success rate of 97.8%, showcasing the versatility and robustness of our approach. Our main contributions can be summarized as follows: - We propose *PPTAgent*, a novel framework that redefines automatic presentation generation as an edit-based workflow guided by reference presentations. - We introduce *PPTEval*, the first comprehensive evaluation framework that assesses presentations across three key dimensions: Content, Design, and Coherence. - We publicly released the *PPTAgent* and *PPTEval* codebase, along with a curated presentation dataset, to facilitate future research in automatic presentation generation.",
                    "medias": []
                }
            ]
        },
        {
            "title": "2 PPTAgent",
            "subsections": [
                {
                    "title": "2.1 Problem Formulation",
                    "content": "PPTAgent is designed to generate an engaging presentation via an edit-based process. We will provide formal definitions for both PPTAgent and the conventional method, illustrating their divergence. The conventional method for creating each slide S can be described in Equation 1, where n represents the number of elements on the slide, and C denotes the source content composed of sections and figures. Each element on the slide, ei, is defined by its type, content, and styling attributes, such as (Textbox, \"Hello\", {border, size, position, ... }). Compared to the conventional method, PPTAgent adopts an edit-based paradigm for creating new slides, addressing challenges in processing spatial relationships and designing styles. This approach generates a sequence of actions to modify existing slides. Within this paradigm, both the input document and the reference presentation serve as inputs. This process can be described as Equation 2, where m represents the number of generated actions. Each action ai represents a line of executable code, and Rj is the reference slide being edited.",
                    "medias": []
                },
                {
                    "title": "2.2 Stage I: Presentation Analysis",
                    "content": "To facilitate presentation generation, we first cluster slides in the reference presentation and extract their content schemas. This structured semantic representation helps LLMs determine which slides to edit and what content to convey in each slide. Slides can be categorized into two main types based on their functionalities: slides that support the structure of the presentation (e.g., opening slides) and slides that convey specific content (e.g., bullet-point slides). We employ different clustering algorithms to effectively cluster slides in the presentation based on their textual or visual characteristics. For structural slides, we leverage LLMs to infer the functional role of each slide and group them accordingly, as these slides often exhibit distinctive textual features. For the remaining slides, which primarily focus on presenting specific content, we employ a hierarchical clustering approach leveraging image similarity. For each cluster, we infer the layout patterns of each cluster using MLLMs. Further details regarding this method can be found in Appendix C. After clustering slides to facilitate the selection of slide references, we further analyzed their content schemas to ensure purposeful alignment of the editing. Given the complexity and fragmentation of real-world slides, we utilized the context perception capabilities of LLMs to extract diverse content schemas. Specifically, we defined an extraction framework where each element is represented by its category, modality, and content. Based on this framework, the schema of each slide was extracted through LLMs' instruction-following and structured output capabilities. Detailed instructions are provided in Appendix E.",
                    "medias": []
                },
                {
                    "title": "2.3 Stage II: Presentation Generation",
                    "content": "In this stage, we begin by generating an outline that specifies the reference slide and relevant content for each slide in the new presentation. For each slide, LLMs iteratively edit the reference slide using interactive executable code actions to complete the generation process. Following human preferences, we instruct LLMs to create a structured outline composed of multiple entries. Each entry specifies the reference slide, relevant document section indices, as well as the title and description of the new slide. By utilizing the planning and summarizing capabilities of LLMs, we provide both the document and semantic information extracted from the reference presentation to generate a coherent and engaging outline for the new presentation, which subsequently orchestrates the generation process. Slide generation is guided by the outline, and the slide generation process iteratively edits a reference slide to produce the new slide. To enable precise manipulation of slide elements, we implement five specialized APIs that allow LLMs to edit, remove, and duplicate text elements, as well as edit and remove visual elements. To further enhance the comprehension of slide structure, inspired by Feng et al. and Tang et al., we convert slides from their raw XML format into an HTML representation, which is more interpretable for LLMs. For each slide, LLMs receive two types of input: text retrieved from the source document based on section indices, and captions of available images. The new slide content is then generated following the guidance of the content schema. Subsequently, LLMs leverage the generated content, HTML representation of the reference slide, and API documentation to produce executable editing actions. These actions are executed in a REPL environment, where the system detects errors during execution and provides real-time feedback for self-correction. The self-correction mechanism leverages intermediate results to iteratively refine the editing actions, enhancing the robustness of the generation process.",
                    "medias": []
                }
            ]
        },
        {
            "title": "3 PPTEval",
            "subsections": [
                {
                    "title": "Introduction",
                    "content": "To address the limitations of existing automated metrics for presentation evaluation, we introduce *PPTEval*, a comprehensive framework for assessing presentation quality from multiple perspectives. The framework provides scores on a 1-to-5 scale and offers detailed feedback to guide the improvement of future presentation generation methods. The overall evaluation process is depicted in Figure 3, with the detailed scoring criteria and examples provided in Appendix B.",
                    "medias": []
                },
                {
                    "title": "Content",
                    "content": "The content dimension evaluates the information presented on the slides, focusing on both text and images. We assess content quality from three perspectives: the amount of information, the clarity and quality of textual content, and the support provided by visual content. High-quality textual content is characterized by clear, impactful text that conveys the proper amount of information. Additionally, images should complement and reinforce the textual content, making the information more accessible and engaging. To evaluate content quality, we employ MLLMs on slide images, as slides cannot be easily comprehended in a plain text format.",
                    "medias": []
                },
                {
                    "title": "Design",
                    "content": "Good design not only captures attention but also enhances content delivery. We evaluate the design dimension based on three aspects: color schemes, visual elements, and overall design. Specifically, the color scheme of the slides should have clear contrast to highlight the content while maintaining harmony. The use of visual elements, such as geometric shapes, can make the slide design more expressive. Finally, good design should adhere to basic design principles, such as avoiding overlapping elements and ensuring that design does not interfere with content delivery.",
                    "medias": []
                },
                {
                    "title": "Coherence",
                    "content": "Coherence is essential for maintaining audience engagement in a presentation. We evaluate coherence based on the logical structure and the contextual information provided. Effective coherence is achieved when the model constructs a captivating storyline, enriched with contextual information that enables the audience to follow the content seamlessly. We assess coherence by analyzing the logical structure and contextual information extracted from the presentation.",
                    "medias": []
                }
            ]
        },
        {
            "title": "4 Experiment",
            "subsections": [
                {
                    "title": "4.1 Dataset",
                    "content": "Existing presentation datasets have two main issues: loss of semantic information and limited diversity. To address these, we introduce *Zenodo10K*, a new dataset sourced from Zenodo, an open digital repository. We have curated 10,448 presentations from this source and made them publicly available. We sampled 50 presentations across five domains to serve as reference presentations and collected 50 documents from the same domains for input. Detailed statistics of the dataset are presented in Table 1.",
                    "medias": [
                        {
                            "markdown_content": "| Domain | Document |  |  | Presentation |  |\n| --- | --- | --- | --- | --- | --- |\n|  | #Chars | #Figs | #Chars | #Figs | #Pages |\n| Culture | 12,708 | 2.9 | 6,585 | 12.8 | 14.3 |\n| Education | 12,305 | 5.5 | 3,993 | 12.9 | 13.9 |\n| Science | 16,661 | 4.8 | 5,334 | 24.0 | 18.4 |\n| Society | 13,019 | 7.3 | 3,723 | 9.8 | 12.9 |\n| Tech | 18,315 | 11.4 | 5,325 | 12.9 | 16.8 |",
                            "markdown_caption": "Table 1: Statistics of the dataset used in our experiments, detailing the number of characters ('#Chars') and figures ('#Figs'), as well as the number of pages ('#Pages').",
                            "path": "test_pdf/table_3d89.png",
                            "caption": "Table: Dataset statistics for various domains, showing the number of characters, figures in documents, and pages in presentations."
                        }
                    ]
                },
                {
                    "title": "4.2 Experimental Settings and Baseline",
                    "content": "We evaluate our method using three state-of-the-art models: GPT-4o-2024-08-06, Qwen2.5-72B-Instruct, and Qwen2-VL-72B-Instruct. Configurations are combinations of a language model (LM) and a vision model (VM). We allow up to two iterations of self-correction per slide generation task, producing 500 presentations per configuration. We use Chen et al. and Wu et al. to compute text and image embeddings. All open-source LLMs are deployed using the VLLM framework on a cluster of 8 NVIDIA A100 GPUs, with a total computational cost of approximately 500 GPU hours. The baseline method, as described in Bandyopadhyay et al., employs a multi-staged end-to-end model to generate narrative-rich presentations. Each configuration generates 50 presentations. Detailed performance comparison is provided in Table 2.",
                    "medias": [
                        {
                            "markdown_content": "| Setting Existing Metrics |  |  |  |  |  | PPTEval |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Language Model | Vision Model | SR(%)\u2191 | PPL\u2193 | FID\u2193 | Content\u2191 | Design\u2191 | Coherence\u2191 | Avg.\u2191 |\n| Baseline |  |  |  |  |  |  |  |  |\n| GPT-4oLM | \u2013 | \u2013 | 110.6 | \u2013 | 2.98 | 2.33 | 3.24 | 2.85 |\n| Qwen2.5LM | \u2013 | \u2013 | 122.4 | \u2013 | 2.96 | 2.37 | 3.28 | 2.87 |\n| PPTAgent |  |  |  |  |  |  |  |  |\n| GPT-4oLM | GPT-4oVM | 97.8 | 459.7 | 7.48 | 3.25 | 3.24 | 4.39 | 3.62 |\n| Qwen2-VLLM | Qwen2-VLVM | 43.0 | 322.3 | 7.32 | 3.13 | 3.34 | 4.07 | 3.51 |\n| Qwen2.5LM | Qwen2-VLVM | 95.0 | 313.9 | 6.20 | 3.28 | 3.27 | 4.48 | 3.67 |\n| Ablation |  |  |  |  |  |  |  |  |\n| PPTAGENT |  | 95.0 | 313.9 | 6.20 | 3.28 | 3.27 | 4.48 | 3.67 |\n| w/o Outline |  | 91.0 | 2304.3 | 6.94 | 3.24 | 3.30 | 3.36 | 3.30 |\n| 164.8 | w/o Schema | 78.8 |  | 7.12 | 3.08 | 3.23 | 4.04 | 3.45 |\n| w/o Structure |  | 92.2 | 189.9 | 7.66 | 3.28 | 3.25 | 3.45 | 3.32 |\n| w/o CodeRender |  | 74.6 | 231.0 | 7.03 | 3.27 | 3.34 | 4.38 | 3.66 |",
                            "markdown_caption": "Table 2: Performance comparison of the baseline, our proposed PPTAgent framework, and its ablation variants. Results are reported using existing metrics\u2014Success Rate (SR), Perplexity (PPL), and FID\u2014as well as our proposed PPTEval metrics, which assess Content, Design, Coherence, and their average score.",
                            "path": "test_pdf/table_a773.png",
                            "caption": "Table: Performance comparison of baseline models, PPTAgent, and its ablation variants using SR, PPL, FID, and PPTEval metrics (Content, Design, Coherence, Avg)."
                        },
                        {
                            "markdown_content": "| Domain | SR (%) | PPL | FID | PPTEval |\n| --- | --- | --- | --- | --- |\n| Culture | 93.0 | 185.3 | 5.00 | 3.70 |\n| Education | 94.0 | 249.0 | 7.90 | 3.69 |\n| Science | 96.0 | 500.6 | 6.07 | 3.56 |\n| Society | 95.0 | 396.8 | 5.32 | 3.59 |\n| Tech | 97.0 | 238.7 | 6.72 | 3.74 |",
                            "markdown_caption": "Table 3: Evaluation results under the configuration of Qwen2-VLLM+Qwen2-VLVM in different domains, using the success rate (SR), PPL, FID, and the average PPTEval score across three evaluation dimensions.",
                            "path": "test_pdf/table_ccfc.png",
                            "caption": "Table: Evaluation metrics (SR, PPL, FID, PPTEval) for Qwen2-VLLM+Qwen2-VLVM across various domains."
                        },
                        {
                            "markdown_content": "![](_page_6_Figure_0.jpeg)",
                            "markdown_caption": "Figure 4: The number of iterative self-corrections required to generate a single slide under different models.",
                            "path": "test_pdf/_page_6_Figure_0.jpeg",
                            "caption": "Chart: Horizontal bar chart comparing iterative corrections for generating slides across three models: gpt-4o, Qwen2.5, and Qwen2-VL. Bars indicate Iter-0, Iter-1, Iter-2, and Failed outcomes on a logarithmic scale."
                        }
                    ]
                },
                {
                    "title": "4.3 Evaluation Metrics",
                    "content": "We evaluated the presentation generation using the following metrics: Success Rate (SR) measures the robustness of the generation task. Perplexity (PPL) measures the likelihood of the language model generating the given sequence. FID measures the similarity between the generated presentation and the exemplar presentation in the feature space. PPTEval measures the comprehensive quality of presentations across three dimensions: coherence, content, and design.",
                    "medias": []
                },
                {
                    "title": "4.4 Result & Analysis",
                    "content": "Table 2 presents the performance comparison between PPTAgent and baseline methods. PPTAgent enhances LLMs' presentation generation capabilities, achieving a remarkable success rate of \u2265 95%. This improvement is attributed to content modifying, streamlined API design, and the code interaction module. Detailed performance of Qwen2.5LM+Qwen2-VLVM across various domains, as illustrated in Table 3, underscores the robustness of our approach. PPTAgent improves overall presentation quality by adopting an edit-based paradigm, allowing elements to inherit well-designed styling attributes. Using GPT-4o, we achieved comprehensive improvements over the baseline in design, coherence, and content dimensions. Open-source LLMs, such as Qwen2.5, can rival GPT-4o in performance.",
                    "medias": []
                },
                {
                    "title": "4.5 Ablation Study",
                    "content": "To understand the impact of each component, we performed ablation studies using four configurations: (1) randomly selecting a slide as the edit target (w/o Outline), (2) omitting structural information during outline generation (w/o Structure), (3) replacing the slide representation (w/o CodeRender), and (4) removing guidance from the slide schema (w/o Schema). Code representation significantly enhances LLMs' comprehension, as removing it leads to a significant drop in success rate. Presentation analysis, including outline and structure, is essential for maintaining logical flow and generating targeted content.",
                    "medias": [
                        {
                            "markdown_content": "| Corelation Content Design Coherence Avg. |  |  |  |  |\n| --- | --- | --- | --- | --- |\n| Pearson | 0.70 | 0.90 | 0.55 | 0.71 |\n| Spearman | 0.73 | 0.88 | 0.57 | 0.74 |",
                            "markdown_caption": "Table 4: The correlation scores between human ratings and LLM ratings under different dimensions (Coherence, Content, Design). All presented data of similarity exhibit a p-value below 0.05, indicating a statistically significant level of confidence.",
                            "path": "test_pdf/table_7d22.png",
                            "caption": "Table: Correlation scores (Pearson and Spearman) between human and LLM ratings across Coherence, Content, and Design dimensions, with average scores and all p-values below 0.05."
                        }
                    ]
                },
                {
                    "title": "4.6 Error Analysis",
                    "content": "Figure 4 illustrates the number of iterations required to generate a slide using different models. GPT-4o exhibits superior self-correction capabilities, while Qwen2.5 encounters fewer errors in the first iteration. Qwen2-VL experiences more frequent errors and has poorer self-correction capabilities. All three models successfully corrected more than half of the errors, demonstrating the effectiveness of the iterative self-correction mechanism.",
                    "medias": [
                        {
                            "markdown_content": "![](_page_7_Figure_0.jpeg)",
                            "markdown_caption": "Figure 5: Correlation heatmap between existing automated evaluation metrics and PPTEval PPT.",
                            "path": "test_pdf/_page_7_Figure_0.jpeg",
                            "caption": "Chart: Heatmap displaying correlation values among four variables: content, vision, ppl, and fid. Color gradient indicates strength and direction of correlations, ranging from -1 to 1. Variables show varying degrees of positive and negative relationships.\n\n"
                        }
                    ]
                },
                {
                    "title": "4.7 Effectiveness of PPTEval",
                    "content": "Despite Chen et al. highlighting the impressive human-like discernment of LLMs in various generation tasks, it remains crucial to assess the correlation between LLM evaluations and human evaluations in the context of presentations. The average Pearson correlation of 0.71 exceeds other evaluation methods, indicating that PPTEval aligns well with human preferences. The heatmap in Figure 5 reveals the limitations of existing metrics in capturing content and design dimensions. PPL captures text fluency but is susceptible to fragmented slide text, and FID quantifies stylistic similarity but does not indicate superior design. These findings underscore the necessity of PPTEval for comprehensive and effective presentation evaluation.",
                    "medias": []
                }
            ]
        },
        {
            "title": "5 Related Works",
            "subsections": [
                {
                    "title": "Automated Presentation Generation",
                    "content": "Recent proposed methods for slide generation can be categorized into rule-based and template-based based on how they handle element placement. Rule-based methods often focus on enhancing textual content but neglect the visual-centric nature of presentations, leading to outputs that lack engagement. Template-based methods rely on pre-designed templates to create visually appealing presentations, but their dependence on extensive manual effort for template annotation significantly limits scalability and flexibility.",
                    "medias": []
                },
                {
                    "title": "LLM Agent",
                    "content": "Numerous studies have explored the potential of LLMs to act as agents assisting humans in a wide array of tasks. For example, they demonstrate the capability to accomplish tasks by generating executable actions and correcting errors based on feedback. Furthermore, an evaluation system that assesses the ability of LLMs to perform multi-turn, multimodal slide editing tasks using APIs has been introduced, which inspired the use of LLMs for complex tasks as proposed in this study.",
                    "medias": []
                },
                {
                    "title": "LLM as a Judge",
                    "content": "LLMs have demonstrated strong capabilities in instruction following and context perception, leading to their widespread use as judges. Further research enhanced LLMs' abilities through external modules and functions, while the feasibility of using multimodal large language models (MLLMs) as judges has been validated. Additionally, a multi-dimensional evaluation framework for multi-turn conversations has been introduced, which inspired the development of the proposed *PPT Eval* PPT.",
                    "medias": []
                }
            ]
        },
        {
            "title": "Conclusion",
            "subsections": [
                {
                    "title": "Conclusion",
                    "content": "In this paper, we introduced PPTAgent, which conceptualizes presentation generation as a two-stage presentation editing task completed through the abilities of LLMs to understand and generate code. This approach leveraged the textual feature and layout patterns to organize slides into different functional groups. Our experiments across data from multiple domains have demonstrated the superiority of our method. Moreover, our proposed PPTEval ensured the assessability of presentations. This research provides a new paradigm for generating slides under unsupervised conditions and offers fresh insights for future work in presentation generation.",
                    "medias": []
                }
            ]
        }
    ],
    "metadata": {
        "title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides",
        "authors": [
            "Hao Zheng",
            "Xinyan Guan",
            "Hao Kong",
            "Jia Zheng",
            "Hongyu Lin",
            "Yaojie Lu",
            "Ben He",
            "Xianpei Han",
            "Le Sun"
        ],
        "affiliations": [
            "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
            "University of Chinese Academy of Sciences",
            "Shanghai Jiexin Technology"
        ],
        "emails": [
            "zhenghao2022@iscas.ac.cn",
            "guanxinyan2022@iscas.ac.cn",
            "zhengjia@iscas.ac.cn",
            "hongyu@iscas.ac.cn",
            "luyaojie@iscas.ac.cn",
            "benhe@iscas.ac.cn",
            "xianpei@iscas.ac.cn",
            "sunle@iscas.ac.cn",
            "haokong@knowuheart.com"
        ],
        "publish_date": "7 Jan 2025",
        "organization": "arXiv",
        "url": "https://arxiv.org/abs/2501.03936v1",
        "presentation_time": "2025-03-19"
    }
}